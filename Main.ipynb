{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa59b95f",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8e8e7b",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0e6528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import EagarTsai as et\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from rl import CustomEnv\n",
    "import json\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df0b0b",
   "metadata": {},
   "source": [
    "## Initialising fixed parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5158a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reading input file to get model parameter settings chosen by the user\"\"\"\n",
    "\n",
    "f = open(\"model_params_train.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "# Assigning the values to variables so they can be used in the training and testing below\n",
    "\n",
    "timestep = data['timestep']                 # Number of timesteps in an episode\n",
    "max_steps = data['maximum_steps']           # Maximum number of timesteps across episodes in a single epoch\n",
    "epochs = data['epochs']                     # Total number of epochs\n",
    "model_alpha = data['model_alpha']           # Learning rate of the Bellmann equation\n",
    "model_gamma = data['model_gamma']           # Discount factor of the Bellmann equation\n",
    "model_epsilon = data['model_epsilon']       # Initial value for epsilon-greedy algorithm\n",
    "store_data = bool(data['store_data'])       # Boolean value whether to store data locally or not\n",
    "\n",
    "# The remaining varaibles dictate the minimum, maximum, and interval values for the three process parameters\n",
    "min_power = data['min_power']\n",
    "max_power = data['max_power']\n",
    "interval_power = data['interval_power']\n",
    "min_speed = data['min_speed']\n",
    "max_speed = data['max_speed']\n",
    "interval_speed = data['interval_speed']\n",
    "min_hatch = data['min_hatch']\n",
    "max_hatch = data['max_hatch']\n",
    "interval_hatch = data['interval_hatch']\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e796667a",
   "metadata": {},
   "source": [
    "## Creating parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17f266b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power = np.arange(min_power, max_power + 1, interval_power)\n",
    "speed = np.arange(min_speed, max_speed + 1, interval_speed)\n",
    "hatch = np.arange(min_hatch, max_hatch + 0.001, interval_hatch)\n",
    "\n",
    "parameters = []\n",
    "for p in power:\n",
    "    for s in speed:\n",
    "        for h in hatch:\n",
    "            parameters.append((p,s,h))\n",
    "            \n",
    "len(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f673a82",
   "metadata": {},
   "source": [
    "## Running training model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8479c48",
   "metadata": {},
   "source": [
    "#### The training model is where the model uses the epsilon greedy algorithm to explore the state space to find the state with the highest reward. The epsilon value comes into play and over time the agent begins exploiting more often, choosing the best action instead of a random one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2a0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pd.DataFrame(columns = ['timesteps', 'test number', 'alpha', 'gamma', 'total steps', 'steps to optimal',\\\n",
    "                                     'optimal state', 'reward', 'reward per episode', 'time taken', 'no. of states visited'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af1c5051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed\n",
      "------------------\n",
      "Episode 2 completed\n",
      "------------------\n",
      "Episode 3 completed\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 completed\n",
      "------------------\n",
      "Episode 5 completed\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 completed\n",
      "------------------\n",
      "Episode 2 completed\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 completed\n",
      "------------------\n",
      "Episode 4 completed\n",
      "------------------\n",
      "Episode 5 completed\n",
      "------------------\n",
      "Episode 1 completed\n",
      "------------------\n",
      "Episode 2 completed\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n",
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 completed\n",
      "------------------\n",
      "Episode 4 completed\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muadh\\Documents\\UoB\\Research Assistant\\Project\\EagarTsai.py:136: RuntimeWarning: invalid value encountered in sqrt\n",
      "  vx = (depth * width * (v ** 2)) / (4 * a * (1.5 * (depth + width / 2) - np.sqrt(depth * width / 2)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 completed\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()         # Beginning timer for the entire training\n",
    "\n",
    "for j in range(0, epochs):               # For each epoch\n",
    "    t1 = time.perf_counter()             # Beginning timer for the epoch\n",
    "      \n",
    "    # Creating instance of the environment\n",
    "    env = CustomEnv(parameters, timestep, model_alpha, model_gamma, model_epsilon)\n",
    "    \n",
    "    episode = 0                          # Initialising episode number to 0\n",
    "    states = set()                       # Total number of states visited in the epoch\n",
    "    states_visited = []                  # Total number of states visited in the episode\n",
    "    epoch_reward = 0                     # Total value of rewards in the epoch\n",
    "    episode_rewards = []                 # Set containing each episode's total reward\n",
    "\n",
    "    # As long as the maximum timesteps in an epoch is not exceeded\n",
    "    while(env.steps < max_steps):\n",
    "        env.reset(timestep)              # Resetting the environment\n",
    "        done = False                     # Setting done to be False so the episode restarts\n",
    "        episode += 1\n",
    "        total_reward = 0                 # Episode reward initialised to zero before start of episode\n",
    "        \n",
    "        while not done:                  # For each episode\n",
    "            reward, done = env.step(power, speed, hatch)\n",
    "            total_reward += reward\n",
    "            epoch_reward += reward\n",
    "            states_visited.append(env.state)\n",
    "            states.add(env.state)               \n",
    "        \n",
    "        print('Episode', episode, ' of epoch', j + 1, 'completed')\n",
    "        #print('States visited in episode ', episode, 'of test', j+1, 'are ', states_visited)\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        print('------------------')\n",
    "        states_visited.clear()\n",
    "    \n",
    "    t2 = time.perf_counter()             # Ending timer for the epoch\n",
    "    time_taken = t2 - t1                 # Calculating time taken for the epoch to run\n",
    "    \n",
    "    row = pd.Series([timestep, j + 1, env.alpha, env.gamma, env.steps, env.optimal_steps, env.optimal_state, \\\n",
    "                     env.rmax, epoch_reward / episode, time_taken, len(states)], \\\n",
    "                    index = train_results.columns)\n",
    "    train_results.loc[len(train_results)] = row\n",
    "    \n",
    "    # If the user opts to store the results, then for each epoch the individual step results and the overall\n",
    "    # train results will be stored in separate excel files.\n",
    "    if store_data:\n",
    "        env.results.to_excel(f'Train Results//Epoch {j + 1} Step Results.xlsx')\n",
    "        train_results.sort_values('reward', ascending = False).to_excel(f'Train Results//Epoch {j + 1} Train Results.xlsx')\n",
    "        \n",
    "end_time = time.perf_counter()           # Ending timer for entire training\n",
    "\n",
    "time_elapsed = end_time - start_time     # Calculating time taken for the whole training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0679709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time is 4.407066221666669 minutes\n"
     ]
    }
   ],
   "source": [
    "print('Elapsed Time is', time_elapsed / 60, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c7332d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exporting results to an Excel sheet'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Exporting results to an Excel sheet\"\"\"\n",
    "\n",
    "#train_results = pd.read_excel('Tugrul Results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94bdaebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Importing results from an Excel sheet'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Importing results from an Excel sheet\"\"\"\n",
    "\n",
    "#train_results.to_excel('Pre-Paper Results.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71846446",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timesteps</th>\n",
       "      <th>test number</th>\n",
       "      <th>alpha</th>\n",
       "      <th>gamma</th>\n",
       "      <th>total steps</th>\n",
       "      <th>steps to optimal</th>\n",
       "      <th>optimal state</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward per episode</th>\n",
       "      <th>time taken</th>\n",
       "      <th>no. of states visited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>2.073876</td>\n",
       "      <td>56.982977</td>\n",
       "      <td>76.588603</td>\n",
       "      <td>652.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>2.073876</td>\n",
       "      <td>41.892755</td>\n",
       "      <td>75.494205</td>\n",
       "      <td>663.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>2.073876</td>\n",
       "      <td>49.482645</td>\n",
       "      <td>86.816980</td>\n",
       "      <td>636.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1610.0</td>\n",
       "      <td>2.073876</td>\n",
       "      <td>61.046617</td>\n",
       "      <td>95.442305</td>\n",
       "      <td>623.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1.991684</td>\n",
       "      <td>58.715699</td>\n",
       "      <td>81.792709</td>\n",
       "      <td>673.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>1.842630</td>\n",
       "      <td>18.194075</td>\n",
       "      <td>82.722810</td>\n",
       "      <td>616.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timesteps  test number  alpha  gamma  total steps  steps to optimal  \\\n",
       "1      200.0          2.0    0.2    0.5       1000.0             376.0   \n",
       "2      200.0          3.0    0.2    0.5       1000.0             183.0   \n",
       "3      200.0          1.0    0.2    0.5       1000.0              51.0   \n",
       "5      200.0          3.0    0.2    0.5       1000.0             174.0   \n",
       "4      200.0          2.0    0.2    0.5       1000.0             242.0   \n",
       "0      200.0          1.0    0.2    0.5       1000.0             975.0   \n",
       "\n",
       "   optimal state    reward  reward per episode  time taken  \\\n",
       "1         1610.0  2.073876           56.982977   76.588603   \n",
       "2         1610.0  2.073876           41.892755   75.494205   \n",
       "3         1610.0  2.073876           49.482645   86.816980   \n",
       "5         1610.0  2.073876           61.046617   95.442305   \n",
       "4         1600.0  1.991684           58.715699   81.792709   \n",
       "0          880.0  1.842630           18.194075   82.722810   \n",
       "\n",
       "   no. of states visited  \n",
       "1                  652.0  \n",
       "2                  663.0  \n",
       "3                  636.0  \n",
       "5                  623.0  \n",
       "4                  673.0  \n",
       "0                  616.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Displaying training details for each epoch\"\"\"\n",
    "\n",
    "train_results.sort_values('reward', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6968be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameter configurations (P, v, h):\n",
      "\n",
      "Epoch 1: (250, 750, 0.0375)\n",
      "Epoch 2: (400, 2325, 0.0375)\n",
      "Epoch 3: (400, 2325, 0.0375)\n",
      "Epoch 4: (400, 2325, 0.0375)\n",
      "Epoch 5: (400, 2100, 0.0375)\n",
      "Epoch 6: (400, 2325, 0.0375)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Printing best parameter configuration from each epoch\"\"\"\n",
    "\n",
    "print(\"Optimal parameter configurations (P, v, h):\\n\")\n",
    "for i in range(len(train_results)):\n",
    "    opt_state = train_results['optimal state'][i]\n",
    "    print(f\"Epoch {i + 1}: {parameters[int(opt_state)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f333785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Storing Q table as an Excel file if required for testing'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Storing Q table as an Excel file if required for testing\"\"\"\n",
    "\n",
    "# df = pd.DataFrame(data=env.qtable)\n",
    "\n",
    "# df = (df.T)\n",
    "\n",
    "# df.to_excel('Qtable_train.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8acc1f",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3864b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"If a specific Qtable is to be used, then import it here, otherwise the one from the train model is used\"\"\"\n",
    "try:\n",
    "    filename = \"Qtable_train.xlsx\"\n",
    "    qtable = pd.read_excel(filename)\n",
    "    print(\"Using imported Q-table\")\n",
    "except:\n",
    "    qtable = env.qtable\n",
    "    print(\"Using Q-table from train model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143fa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reading input file to get model parameter settings chosen by the user for testing\"\"\"\n",
    "\n",
    "f = open(\"model_params_test.json\")\n",
    "data = json.load(f)\n",
    "\n",
    "# Assigning the values to variables so they can be used in the training and testing below\n",
    "\n",
    "timestep = data['timestep']                 # Number of timesteps in an episode\n",
    "max_steps = data['maximum_steps']           # Maximum number of timesteps across episodes in a single epoch\n",
    "epochs = data['epochs']                     # Total number of epochs\n",
    "model_alpha = data['model_alpha']           # Learning rate of the Bellmann equation\n",
    "model_gamma = data['model_gamma']           # Discount factor of the Bellmann equation\n",
    "model_epsilon = data['model_epsilon']       # Initial value for epsilon-greedy algorithm\n",
    "store_data = bool(data['store_data'])       # Boolean value whether to store data locally or not\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame(columns = ['timestep', 'test number', 'alpha', 'gamma', 'total steps', 'steps to optimal',\\\n",
    "                                     'optimal state', 'reward', 'reward per episode', 'time taken', \\\n",
    "                                        'number of states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384a7d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a table to record only the reward for each episode\n",
    "test_reward_table = pd.DataFrame(index = np.arange(1, 6, 1))\n",
    "test_reward_table.index.name = 'Episode Number'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986502b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()         # Beginning timer for the entire training\n",
    "\n",
    "for j in range(0,epochs):                # For each epoch\n",
    "    t1 = time.perf_counter()             # Beginning timer for the epoch\n",
    "    \n",
    "    # Creating instance of the environment\n",
    "    env_test = CustomEnv(parameters, timestep, model_alpha, model_gamma, model_epsilon, qtable = qtable)\n",
    "    \n",
    "    episode = 0                          # Initialising episode number to 0\n",
    "    states = set()                       # Total number of states visited in the epoch\n",
    "    states_visited = []                  # Total number of states visited in the episode\n",
    "    epoch_reward = 0                     # Total value of rewards in the epoch\n",
    "    episode_rewards = []                 # Set containing each episode's total reward\n",
    "\n",
    "    # As long as the maximum timesteps in an epoch is not exceeded\n",
    "    while(env_test.steps < max_steps):\n",
    "        env_test.reset(timestep)         # Resetting the environment\n",
    "        done = False                     # Setting done to be False so the episode restarts\n",
    "        episode += 1                     \n",
    "        total_reward = 0                 # Episode reward initialisd to zero before start of episode\n",
    "        \n",
    "        while not done:                  # For each episode\n",
    "            reward, done = env_test.step(power, speed, hatch, test = True)\n",
    "            total_reward += reward\n",
    "            epoch_reward += reward\n",
    "            states_visited.append(env_test.state)\n",
    "            states.add(env_test.state)\n",
    "        print('States visited in episode ', episode, 'of test', j+1, 'are ', states_visited)\n",
    "        \n",
    "        episode_rewards.append(total_reward)  \n",
    "        print('------------------')\n",
    "        states_visited.clear()\n",
    "    \n",
    "    t2 = time.perf_counter()             # Ending timer for the epoch\n",
    "    time_taken = t2 - t1                 # Calculating time taken for the epoch to run\n",
    "    \n",
    "    test_reward_table[f'test {j + 1} rewards'] = episode_rewards\n",
    "    row = pd.Series([timestep, j + 1, env_test.alpha, env_test.gamma, env_test.steps, env_test.optimal_steps,\\\n",
    "                     env_test.optimal_state, env_test.rmax, epoch_reward / episode, time_taken, \\\n",
    "                     len(states)], index = test_results.columns)\n",
    "    test_results.loc[len(test_results)] = row\n",
    "    \n",
    "    # If the user opts to store the results, then for each epoch the individual step results and the overall\n",
    "    # train results will be stored in separate excel files.\n",
    "    if store_data:\n",
    "        env.results.to_excel(f'Test Results//Epoch {j + 1} Step Results.xlsx')\n",
    "        test_results.sort_values('reward', ascending = False).to_excel(f'Test Results//Epoch {j + 1} Test Results.xlsx')\n",
    "\n",
    "    \n",
    "end_time = time.perf_counter()           # Ending timer for entire training\n",
    "\n",
    "time_elapsed = end_time - start_time     # Calculating time taken for the whole training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ce676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Elapsed Time is', time_elapsed / 60, 'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_reward_table.to_excel('Test Reward table 3.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4251a7bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_results.sort_values('reward', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b9387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Printing best parameter configuration from each epoch\"\"\"\n",
    "\n",
    "print(\"Optimal parameter configurations (P, v, h):\\n\")\n",
    "for i in range(test_results):\n",
    "    opt_state = test_results['optimal state'][i]\n",
    "    print(f\"Epoch {i + 1}: {parameters[int(opt_state)]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
